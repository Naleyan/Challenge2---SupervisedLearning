{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de920aa7",
   "metadata": {},
   "source": [
    "##  Predict your coarse-grained genre "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d3d6e3",
   "metadata": {},
   "source": [
    "4 genres : \n",
    "\n",
    "\n",
    "-  Pop/Rock : Rock, Pop, Electronic (musique populaire et électro).\n",
    "- Urbain/Black music : Hip-Hop, Soul-RnB, Blues (genres liés aux musiques afro‑américaines modernes).\n",
    "- Acoustique / Traditionnel : Folk, Country, Old-Time / Historic, International, Easy Listening (musiques plus “roots”, acoustiques ou de tradition locale).\n",
    "- Spécialisé / Instrumental : Instrumental, Classical, Jazz, Spoken, Experimental (musiques plutôt instrumentales, “art” ou parlées).​"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76513f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2891e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"df_phase1.csv\")\n",
    "df_classif = pd.read_csv(\"df_classif_phase1.csv\")\n",
    "df_audio = pd.read_csv(\"df_audio_phase1.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b9b197c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genre_coarse\n",
      "Pop/Rock      23089\n",
      "Spécialisé    13691\n",
      "Acoustique     4309\n",
      "Urbain         3544\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1. Définir une taxonomie \"coarse\" à 4 genres\n",
    "\n",
    "coarse_map = {\n",
    "    \"Rock\": \"Pop/Rock\",\n",
    "    \"Pop\": \"Pop/Rock\",\n",
    "    \"Electronic\": \"Pop/Rock\",\n",
    "\n",
    "    \"Hip-Hop\": \"Urbain\",\n",
    "    \"Soul-RnB\": \"Urbain\",\n",
    "    \"Blues\": \"Urbain\",\n",
    "\n",
    "    \"Folk\": \"Acoustique\",\n",
    "    \"Country\": \"Acoustique\",\n",
    "    \"Old-Time / Historic\": \"Acoustique\",\n",
    "    \"International\": \"Acoustique\",\n",
    "    \"Easy Listening\": \"Acoustique\",\n",
    "\n",
    "    \"Instrumental\": \"Spécialisé\",\n",
    "    \"Classical\": \"Spécialisé\",\n",
    "    \"Jazz\": \"Spécialisé\",\n",
    "    \"Spoken\": \"Spécialisé\",\n",
    "    \"Experimental\": \"Spécialisé\",\n",
    "}\n",
    "\n",
    "df_classif[\"genre_coarse\"] = df_classif[\"genre_top\"].map(coarse_map)\n",
    "print(df_classif[\"genre_coarse\"].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f764431a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features utilisées : ['log_duration', 'log_listens', 'log_favorites', 'favorites_per_listen', 'tempo', 'energy', 'danceability']\n",
      "X shape : (44633, 7)  | y shape : (44633,)\n"
     ]
    }
   ],
   "source": [
    "# 2. Encoder la cible coarse\n",
    "le_coarse = LabelEncoder()\n",
    "df_classif[\"genre_coarse_encoded\"] = le_coarse.fit_transform(df_classif[\"genre_coarse\"])\n",
    "\n",
    "# 2b. Définir un set de features de base\n",
    "feature_cols = [\n",
    "    \"log_duration\", \"log_listens\", \"log_favorites\",\n",
    "    \"favorites_per_listen\",\n",
    "    \"tempo\", \"energy\", \"danceability\"\n",
    "]\n",
    "feature_cols = [c for c in feature_cols if c in df_classif.columns]\n",
    "\n",
    "X = df_classif[feature_cols].fillna(0)\n",
    "y = df_classif[\"genre_coarse_encoded\"]\n",
    "\n",
    "print(\"Features utilisées :\", feature_cols)\n",
    "print(\"X shape :\", X.shape, \" | y shape :\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5510fdbb",
   "metadata": {},
   "source": [
    "on a 44 633 morceaux, 7 features et 4 classes coarse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6082a20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train X : (35706, 7)  | Train y : (35706,)\n",
      "Test  X : (8927, 7)  | Test y : (8927,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train X :\", X_train.shape, \" | Train y :\", y_train.shape)\n",
    "print(\"Test  X :\", X_test.shape,  \" | Test y :\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5d4d70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LogReg ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.409     0.055     0.096       862\n",
      "           1      0.565     0.907     0.696      4618\n",
      "           2      0.659     0.335     0.444      2738\n",
      "           3      0.167     0.001     0.003       709\n",
      "\n",
      "    accuracy                          0.577      8927\n",
      "   macro avg      0.450     0.324     0.310      8927\n",
      "weighted avg      0.547     0.577     0.506      8927\n",
      "\n",
      "\n",
      "=== kNN ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.244     0.036     0.063       862\n",
      "           1      0.581     0.819     0.680      4618\n",
      "           2      0.573     0.467     0.515      2738\n",
      "           3      0.239     0.023     0.041       709\n",
      "\n",
      "    accuracy                          0.572      8927\n",
      "   macro avg      0.409     0.336     0.325      8927\n",
      "weighted avg      0.519     0.572     0.519      8927\n",
      "\n",
      "\n",
      "=== RandomForest ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.243     0.128     0.167       862\n",
      "           1      0.594     0.712     0.647      4618\n",
      "           2      0.528     0.504     0.516      2738\n",
      "           3      0.227     0.103     0.142       709\n",
      "\n",
      "    accuracy                          0.543      8927\n",
      "   macro avg      0.398     0.362     0.368      8927\n",
      "weighted avg      0.511     0.543     0.521      8927\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"LogReg\": LogisticRegression(max_iter=1000, n_jobs=-1),\n",
    "    \"kNN\": KNeighborsClassifier(n_neighbors=15),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=None,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "}\n",
    "\n",
    "for name, clf in models.items():\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7988915f",
   "metadata": {},
   "source": [
    "- Les trois modèles arrivent à un accuracy autour de 0.55–0.58, donc mieux qu’au hasard mais encore loin d’être parfait, et tous ont tendance à favoriser la classe majoritaire (classe 1 : Pop/Rock) avec un recall très élevé pour celle‑ci.\n",
    "\n",
    "- Les classes minoritaires sont peu reconnues : les recalls sont proches de 0, ce qui montre qu’il y a encore un gros déséquilibre entre les 4 genres coarse et qu’il faudra probablement tester un modèle plus puissant (SVM, boosting) pour mieux capturer ces catégories.\n",
    "\n",
    "​"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f84dcb8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SVM_RBF ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nada\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\Nada\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\Nada\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000       862\n",
      "           1      0.518     1.000     0.683      4618\n",
      "           2      0.913     0.008     0.015      2738\n",
      "           3      0.000     0.000     0.000       709\n",
      "\n",
      "    accuracy                          0.519      8927\n",
      "   macro avg      0.358     0.252     0.174      8927\n",
      "weighted avg      0.548     0.519     0.358      8927\n",
      "\n",
      "\n",
      "=== GradBoost ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.553     0.090     0.156       862\n",
      "           1      0.595     0.849     0.700      4618\n",
      "           2      0.623     0.487     0.547      2738\n",
      "           3      0.707     0.058     0.107       709\n",
      "\n",
      "    accuracy                          0.602      8927\n",
      "   macro avg      0.620     0.371     0.377      8927\n",
      "weighted avg      0.609     0.602     0.553      8927\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models_more = {\n",
    "    \"SVM_RBF\": SVC(kernel=\"rbf\", C=3, gamma=\"scale\"),\n",
    "    \"GradBoost\": GradientBoostingClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "for name, clf in models_more.items():\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150a8982",
   "metadata": {},
   "source": [
    "- Le SVM se comporte très mal : il prédit presque tout en classe 1 (Pop/Rock), avec un recall de 1.0 pour cette classe mais quasi 0 pour les autres, ce qui donne une accuracy moyenne (0.52) mais une macro‑F1 catastrophique, donc on écarte le modèle.\n",
    "\n",
    "​- Le Gradient Boosting est meilleur : l’accuracy monte autour de 0.60, les classes 1 et 2 sont plutôt bien reconnues, mais les classes minoritaires (0 et 3) restent difficiles avec des recalls faibles, ce qui confirme que le regroupement en 4 genres aide un peu, mais que le déséquilibre de classes reste un problème important"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7f6acf",
   "metadata": {},
   "source": [
    "### Conclusion!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fbdbc8",
   "metadata": {},
   "source": [
    "- La taxonomie genre_coarse regroupe les 16 genres initiaux en 4 familles (Pop/Rock, Urbain, Acoustique, Spécialisé), ce qui réduit la complexité du problème tout en gardant une interprétation musicale claire.\n",
    "\n",
    "- Sur ces 4 classes, les modèles simples (LogReg, kNN, RandomForest) atteignent une accuracy autour de 0.55–0.58, avec un bon recall pour la classe majoritaire mais des performances limitées sur les genres minoritaires.\n",
    "\n",
    "​- Le Gradient Boosting obtient les meilleurs résultats avec une accuracy ≈ 0.60, montrant que la réduction du nombre de classes améliore légèrement la généralisation par rapport à la Task 1, même si le déséquilibre entre familles de genres reste un frein important. (à vérifier avec la TASK1)\n",
    "​"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": "mon_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
